## Pergunta 1: "Tente valores diferentes do argumento num_examples na função load_data_nmt. Como isso afeta os tamanhos do vocabulário do idioma de origem e do idioma de destino?"

A primeira pergunta explora como diferentes valores para o argumento num_examples na função load_data_nmt afetam os tamanhos dos vocabulários dos idiomas de origem (inglês) e destino (francês). Com base nos resultados do notebook, verificamos que o tamanho dos vocabulários aumenta proporcionalmente ao número de exemplos utilizados. Por exemplo, quando num_examples foi definido como 600, o vocabulário de inglês consistia em 10.012 palavras, enquanto o vocabulário de francês continha 14.592 palavras. Reduzindo num_examples para 100, o tamanho dos vocabulários foi significativamente menor para ambos os idiomas. Esse comportamento ocorre porque, ao aumentar o número de exemplos, mais palavras únicas são incluídas, ampliando o vocabulário. Por outro lado, com menos exemplos, menos palavras são vistas, resultando em um vocabulário menor. Esses resultados indicam que o tamanho do vocabulário é diretamente influenciado pela quantidade de dados utilizados, o que pode impactar o desempenho do modelo, influenciando tanto sua capacidade de generalização quanto o tempo de treinamento.

## Pergunta 2: "O texto em alguns idiomas, como chinês e japonês, não tem indicadores de limite de palavras (por exemplo, espaço). A tokenização em nível de palavra ainda é uma boa ideia para esses casos? Por que ou por que não?"

A segunda pergunta trata da adequação da tokenização em nível de palavra para idiomas como chinês e japonês, que não possuem espaços para delimitar palavras. Para esses idiomas, a tokenização em nível de palavra não é recomendada. O chinês e o japonês são caracterizados por sequências contínuas de caracteres, onde cada caractere pode representar uma palavra completa ou parte de uma palavra, sem um delimitador claro como um espaço. Isso torna a tokenização em nível de palavra ineficaz, pois não consegue capturar com precisão as unidades semânticas. Em vez disso, técnicas como a tokenização baseada em caracteres ou ferramentas especializadas, como Jieba para chinês e MeCab para japonês, são preferíveis. Essas ferramentas aplicam métodos estatísticos ou de aprendizado de máquina para segmentar as palavras com precisão. A literatura em processamento de linguagem natural e o próprio texto "Dive into Deep Learning" sugerem que métodos como a segmentação subword ou a tokenização de caracteres são mais eficazes para esses idiomas, permitindo uma representação linguística mais adequada. Portanto, a tokenização em nível de palavra não é ideal para chinês e japonês; métodos que lidam com a segmentação adequada de palavras nesses idiomas são essenciais para obter bons resultados em tarefas como tradução automática.

